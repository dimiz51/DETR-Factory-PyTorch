{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing and training a Detection Transformer (DETR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import ops\n",
    "\n",
    "\n",
    "# Import the custom COCO dataset loader\n",
    "from dataloaders.coco_od_pytorch import TorchCOCOLoader, collate_fn\n",
    "from models.detr import DETR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set the experiment configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch size for dataloaders and image size for model/pre-processing\n",
    "BATCH_SIZE = 4\n",
    "IMAGE_SIZE = 480\n",
    "MAX_OBJECTS = 100\n",
    "FREEZE_BACKBONE = True\n",
    "EPOCHS = 150\n",
    "LOG_FREQUENCY = 5 # Training-time losses will be logged according to this frequency\n",
    "SAVE_FREQUENCY = 20 # Model weights will be saved according to this frequency\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # Training device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a PyTorch Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: You can load the COCO dataset infromation or any other from the available datasets \n",
    "#       if it's in the DATASET_CLASSES class map. This map is a lookup dictionary with the\n",
    "#       dataset name as key where each instance has the following attributes:\n",
    "#           - \"class_names\" : The list of class names\n",
    "#           - \"empty_class_id\": The ID of the class to be treated as the \"empty\" class for boxes\n",
    "#           - \"links\": Contains some sort of link to download the dataset\n",
    "# NOTE: All the available datasets are listed in the project README file.\n",
    "from datasets.info import DATASET_CLASSES\n",
    "CLASSES = DATASET_CLASSES[\"people_hq\"][\"class_names\"]\n",
    "EMPTY_CLASS_ID = DATASET_CLASSES[\"people_hq\"][\"empty_class_id\"]\n",
    "\n",
    "\n",
    "# Or explicitly set the  class labels/empty class ID for your custom dataset if its not added to the DATASET_CLASSES map...\n",
    "# CLASSES = [\"N/A\", \"something\"]\n",
    "# EMPTY_CLASS_ID = 0 # ID of the dataset classes to treat as \"empty\" class\n",
    "\n",
    "\n",
    "# Load and COCO dataset (adjust the paths accordingly)\n",
    "coco_ds_train = TorchCOCOLoader(\n",
    "    '../../DL_Datasets/data/people_hq/train',\n",
    "    '../../DL_Datasets/data/people_hq/train/_annotations.coco.json',\n",
    "    max_boxes=MAX_OBJECTS,\n",
    "    empty_class_id=EMPTY_CLASS_ID,\n",
    "    image_size=IMAGE_SIZE,\n",
    "    augment=True\n",
    ")\n",
    "\n",
    "coco_ds_val = TorchCOCOLoader(\n",
    "    '../../DL_Datasets/data/people_hq/valid',\n",
    "    '../../DL_Datasets/data/people_hq/valid/_annotations.coco.json',\n",
    "    max_boxes=MAX_OBJECTS,\n",
    "    empty_class_id=EMPTY_CLASS_ID,\n",
    "    image_size=IMAGE_SIZE,\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    coco_ds_train, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    coco_ds_val, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "print(f\"Training dataset size: {len(coco_ds_train)}\")\n",
    "print(f\"Validation dataset size: {len(coco_ds_val)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot some samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from utils.visualizers import DETRBoxVisualizer\n",
    "\n",
    "# Create a visualizer\n",
    "visualizer = DETRBoxVisualizer(class_labels= CLASSES,\n",
    "                               empty_class_id=0)\n",
    "\n",
    "# Visualize batches\n",
    "dataloader_iter = iter(train_loader)\n",
    "for i in range(1):\n",
    "    input_, (classes, boxes, masks, _) = next(dataloader_iter)\n",
    "    fig = plt.figure(figsize=(10, 10), constrained_layout=True)\n",
    "\n",
    "    for ix in range(4):\n",
    "        t_cl = classes[ix]\n",
    "        t_bbox = boxes[ix]\n",
    "        mask = masks[ix].bool()\n",
    "\n",
    "        # Filter padded classes/boxes using the binary mask...\n",
    "        t_cl = t_cl[mask]\n",
    "        t_bbox = t_bbox[mask] * IMAGE_SIZE\n",
    "\n",
    "        # Convert to x1y1x2y2 for visualization and denormalize boxes..\n",
    "        t_bbox = ops.box_convert(\n",
    "            t_bbox, in_fmt='cxcywh', out_fmt='xyxy')\n",
    "        \n",
    "        im = input_[ix]\n",
    "\n",
    "        ax = fig.add_subplot(2, 2, ix+1)\n",
    "        visualizer._visualize_image(im, t_bbox, t_cl, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the DETR model\n",
    "\n",
    "**Note**: If you decide to start from pre-trained weights the `n_classes` argument should\n",
    "match the number of classes the pre-trained model was trained on (e.g. if trained on \"COCO\" then it should be 92 classes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We do instantiate the model with the COCO dataset parameters in order to load pre-trained weights\n",
    "# to fine-tune on a new dataset with...\n",
    "detr_model = DETR(\n",
    "    d_model=256, n_classes=92, n_tokens=225, \n",
    "    n_layers=6, n_heads=8, n_queries=MAX_OBJECTS, use_frozen_bn=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load pre-trained weights as a starting point to explore Transfer Learning (optional)\n",
    "\n",
    "Using pre-trained weights can significantly speed up training as the model doesn't start from 0.\n",
    "\n",
    "Training DETR from scratch might take significant time even with enough GPU horsepower, while with\n",
    "fine-tuning you can get somewhat decent results with 100-150 epochs depending on your dataset size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT_PATH = \"<YOUR_DETR_WEIGHTS.pt>\"\n",
    "\n",
    "# Load the checkpoint\n",
    "checkpoint = torch.load(CHECKPOINT_PATH, map_location=torch.device(\"cpu\"))\n",
    "\n",
    "# Load the weights into the model\n",
    "# We don't use strict matching as you might want to use FrozenBatchNorm2D...\n",
    "# Some pre-trained weights come from trainings using BatchNorm2D\n",
    "print(detr_model.load_state_dict(checkpoint['state'], strict=False))\n",
    "\n",
    "# Adapt the class prediction head to our new dataset\n",
    "detr_model.linear_class = nn.Linear(detr_model.linear_class.in_features, len(CLASSES))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.trainer import DETRTrainer\n",
    "\n",
    "# Create a trainer for DETR\n",
    "trainer = DETRTrainer(model = detr_model,\n",
    "                      train_loader= train_loader,\n",
    "                      val_loader=val_loader,\n",
    "                      device=device,\n",
    "                      epochs=EPOCHS,\n",
    "                      batch_size=BATCH_SIZE,\n",
    "                      log_freq=5,\n",
    "                      save_freq=SAVE_FREQUENCY,\n",
    "                      freeze_backbone= FREEZE_BACKBONE,\n",
    "                      num_queries=MAX_OBJECTS,\n",
    "                      empty_class_id=EMPTY_CLASS_ID)\n",
    "\n",
    "# Start the training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the training metrics and save the plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.visualize_losses(save_dir = \"./\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load fine-tuned model and test inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.visualizers import DETRBoxVisualizer\n",
    "\n",
    "WEIGHS_PATH = \"<YOUR_DETR_WEIGHTS>\"\n",
    "INFERENCE_DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Trained on your custom dataset\n",
    "detr_model = DETR(\n",
    "    d_model=256, n_classes=len(CLASSES), n_tokens=225, \n",
    "    n_layers=6, n_heads=8, n_queries=MAX_OBJECTS, use_frozen_bn=True\n",
    ").to(INFERENCE_DEVICE)\n",
    "\n",
    "# Load the checkpoint\n",
    "print(detr_model.load_state_dict(torch.load(WEIGHS_PATH, map_location=torch.device(INFERENCE_DEVICE)), strict=False))\n",
    "\n",
    "# Run inference and check results\n",
    "visualizer = DETRBoxVisualizer(class_labels= CLASSES,\n",
    "                               empty_class_id=EMPTY_CLASS_ID)\n",
    "\n",
    "# This will always run inference on a GPU if one is available...\n",
    "for i in range(2):\n",
    "    visualizer.visualize_validation_inference(detr_model, coco_ds_val, collate_fn=collate_fn, batch_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Or run inference on a video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualizer.visualize_video_inference(\n",
    "    model=detr_model,\n",
    "    video_path=\"<YOUR_VIDEO_PATH>\",\n",
    "    save_dir= \"./\",\n",
    "    image_size=480,\n",
    "    batch_size=4,\n",
    "    nms_threshold=0.5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the trained model using the COCO API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.evaluator import DETREvaluator\n",
    "\n",
    "evaluator = DETREvaluator(detr_model, coco_ds_val, device, EMPTY_CLASS_ID, collate_fn, batch_size=4)\n",
    "stats = evaluator.evaluate()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

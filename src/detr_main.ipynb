{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing and training a Detection Transformer (DETR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import ops\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Import the custom COCO dataset loader\n",
    "from dataloaders.coco_od_pytorch import TorchCOCOLoader, collate_fn\n",
    "from models.detr import DETR\n",
    "from models.losses.sample_detr_loss import compute_sample_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a PyTorch Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 4\n",
    "IMAGE_SIZE = 480\n",
    "EMPTY_CLASS_ID = 0 # ID of the dataset classes to treat as \"empty\" class\n",
    "\n",
    "# Class labels for \"vehicles dataset\"\n",
    "CLASSES = [\"N/A\", \"vehicle\"]\n",
    "\n",
    "    \n",
    "\n",
    "# Load the tiny COCO dataset\n",
    "coco_ds_train = TorchCOCOLoader(\n",
    "    '../data/vehicles_dataset/train',\n",
    "    '../data/vehicles_dataset/train/_vehicle_annotations.json'\n",
    ")\n",
    "\n",
    "coco_ds_val = TorchCOCOLoader(\n",
    "    '../data/vehicles_dataset/valid',\n",
    "    '../data/vehicles_dataset/valid/_vehicle_annotations.json'\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    coco_ds_train, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    coco_ds_val, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "print(f\"Training dataset size: {len(coco_ds_train)}\")\n",
    "print(f\"Validation dataset size: {len(coco_ds_val)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot some samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from utils.visualizer import DETRBoxVisualizer\n",
    "\n",
    "# Create a visualizer\n",
    "visualizer = DETRBoxVisualizer(class_labels= CLASSES,\n",
    "                               empty_class_id=0)\n",
    "\n",
    "# Visualize batches\n",
    "dataloader_iter = iter(train_loader)\n",
    "for i in range(1):\n",
    "    input_, (classes, boxes) = next(dataloader_iter)\n",
    "    fig = plt.figure(figsize=(10, 10), constrained_layout=True)\n",
    "\n",
    "    for ix in range(4):\n",
    "        t_cl = classes[ix]\n",
    "        t_bbox = boxes[ix]\n",
    "\n",
    "        t_bbox = ops.box_convert(\n",
    "            t_bbox * IMAGE_SIZE, in_fmt='cxcywh', out_fmt='xyxy')\n",
    "        \n",
    "        im = input_[ix]\n",
    "\n",
    "        ax = fig.add_subplot(2, 2, ix+1)\n",
    "        visualizer._visualize_image(im, t_bbox, t_cl, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the DETR model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We do instantiate the model with the COCO dataset parameters in order to load pre-trained weights\n",
    "# to fine-tune on a new dataset with...\n",
    "detr_model = DETR(\n",
    "    d_model=256, n_classes=92, n_tokens=225, \n",
    "    n_layers=6, n_heads=8, n_queries=100\n",
    ")\n",
    "\n",
    "# Inspect shapes of outputs from the last layer\n",
    "x = torch.randn((1, 3, 480, 480))\n",
    "outs = detr_model(x)\n",
    "pred_cl, pred_boxes = outs['layer_5'].values()\n",
    "\n",
    "print()\n",
    "print(\"*****************************************\")\n",
    "print(f\"Predicted Classes shape: {pred_cl.shape}\")\n",
    "print(f\"Predicted Boxes shape: {pred_boxes.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load pre-trained weights as a starting point "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT_PATH = \"../weights/model_ep150.pt\" # COCO weights\n",
    "\n",
    "# Load the checkpoint\n",
    "checkpoint = torch.load(CHECKPOINT_PATH, map_location=torch.device(\"cpu\"))\n",
    "\n",
    "# Load the weights into the model\n",
    "print(detr_model.load_state_dict(checkpoint['state']))\n",
    "\n",
    "# Adapt the class prediction head to our new dataset\n",
    "detr_model.linear_class = nn.Linear(detr_model.linear_class.in_features, len(CLASSES))\n",
    "\n",
    "# Verify output shapes with the new configurations\n",
    "x = torch.randn((1, 3, 480, 480))\n",
    "outs = detr_model(x)\n",
    "pred_cl, pred_boxes = outs['layer_5'].values()\n",
    "\n",
    "print()\n",
    "print(\"*****************************************\")\n",
    "print(f\"Predicted Classes shape: {pred_cl.shape}\")\n",
    "print(f\"Predicted Boxes shape: {pred_boxes.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "\n",
    "CUDA_ENABLED = False\n",
    "FREEZE_BACKBONE = True\n",
    "\n",
    "# Enable GPU if available\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Model moved to GPU for training...\")\n",
    "    detr_model.cuda()\n",
    "    CUDA_ENABLED = True\n",
    "else:\n",
    "    print(\"No GPU found, training will start on CPU...\")\n",
    "\n",
    "# DETR in the offical paper is trained with different learning rates for backbone and Transformer/prediction heads.\n",
    "# In this case we will freeze the backbone and train only the Transformer/prediction heads as it's already pre-trained...\n",
    "backbone_params = [\n",
    "    p for n, p in detr_model.named_parameters() if 'backbone.' in n]\n",
    "\n",
    "# Freeze backbone\n",
    "if FREEZE_BACKBONE:\n",
    "    for p in detr_model.backbone.parameters():\n",
    "        p.requires_grad = False\n",
    "print(f\"CNN backbone is frozen for training: {FREEZE_BACKBONE}\")\n",
    "\n",
    "transformer_params = [\n",
    "    p for n, p in detr_model.named_parameters() if 'backbone.' not in n]\n",
    "\n",
    "optimizer = AdamW([\n",
    "    {'params': transformer_params, 'lr': 1e-5},\n",
    "], weight_decay=1e-4)\n",
    "\n",
    "\n",
    "# Log the number of parameters\n",
    "nparams = sum([p.nelement() for p in detr_model.parameters()]) / 1e6\n",
    "print(f'DETR trainable parameters: {nparams:.1f}M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure saving directory exists for the model checkpoints...\n",
    "![ ! -d ckpts ] && mkdir ckpts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the model to training mode\n",
    "torch.set_grad_enabled(True)\n",
    "detr_model.train()\n",
    "\n",
    "EPOCHS = 30\n",
    "LOG_FREQUENCY = 10\n",
    "SAVE_FREQUENCY = 20\n",
    "NUM_BATCHES = len(train_loader)\n",
    "\n",
    "losses = []\n",
    "class_losses = []\n",
    "box_losses = []\n",
    "giou_losses = []\n",
    "\n",
    "hist = []\n",
    "\n",
    "# More detailed logs for all losses(tuple: (class, bbox, giou))\n",
    "hist_detail = []\n",
    "\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    for batch_idx, (input_, (tgt_cl, tgt_bbox)) in enumerate(train_loader):\n",
    "        input_ = input_.cuda()\n",
    "        tgt_cl = tgt_cl.cuda()\n",
    "        tgt_bbox = tgt_bbox.cuda()\n",
    "        \n",
    "        outs = detr_model(input_)\n",
    "        \n",
    "        loss = torch.Tensor([0]).cuda()\n",
    "\n",
    "        # For detailed logging of all the losses\n",
    "        loss_class_batch = 0.0\n",
    "        loss_bbox_batch = 0.0\n",
    "        loss_giou_batch = 0.0\n",
    "\n",
    "        for name, out in outs.items(): \n",
    "            out['bbox'] = out['bbox'].sigmoid()\n",
    "            \n",
    "            for o_bbox, t_bbox, o_cl, t_cl in zip(\n",
    "                out['bbox'], tgt_bbox, out['cl'], tgt_cl):\n",
    "        \n",
    "                loss_class, loss_bbox, loss_giou = compute_sample_loss(\n",
    "                    o_bbox, t_bbox, o_cl, t_cl, empty_class_id=EMPTY_CLASS_ID)\n",
    "                \n",
    "                sample_loss = 1 * loss_class + 5 * loss_bbox + 2 * loss_giou\n",
    "                \n",
    "                loss += sample_loss / BATCH_SIZE / len(outs)\n",
    "\n",
    "                # Track individual losses per batch\n",
    "                loss_class_batch += loss_class.item() / BATCH_SIZE / len(outs)\n",
    "                loss_bbox_batch += loss_bbox.item() / BATCH_SIZE / len(outs)\n",
    "                loss_giou_batch += loss_giou.item() / BATCH_SIZE / len(outs)\n",
    "            \n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        \n",
    "        # Clip gradient norms\n",
    "        nn.utils.clip_grad_norm_(detr_model.parameters(), .1)\n",
    "        optimizer.step()\n",
    "        \n",
    "        losses.append(loss.item())\n",
    "        class_losses.append(loss_class_batch)\n",
    "        box_losses.append(loss_bbox_batch)\n",
    "        giou_losses.append(loss_giou_batch)\n",
    "\n",
    "    # Logging every 10 epochs\n",
    "    if (epoch + 1) % LOG_FREQUENCY == 0:\n",
    "        # Get the epoch losses (averaged from batch losses)\n",
    "        loss_avg = np.mean(losses[-NUM_BATCHES:])\n",
    "        epoch_loss_class = np.mean(class_losses[-NUM_BATCHES:])\n",
    "        epoch_loss_bbox = np.mean(box_losses[-NUM_BATCHES:])\n",
    "        epoch_loss_giou = np.mean(giou_losses[-NUM_BATCHES:])\n",
    "\n",
    "        # Log the losses\n",
    "        print(f'Epoch: {epoch+1}/{EPOCHS}, DETR Loss: {loss_avg:.4f}')\n",
    "        print(f'→ Class Loss: {epoch_loss_class:.4f}, BBox Loss: {epoch_loss_bbox:.4f}, GIoU Loss: {epoch_loss_giou:.4f}')\n",
    "        \n",
    "        # Save history per epoch...\n",
    "        hist.append(loss_avg)\n",
    "        hist_detail.append((epoch_loss_class, epoch_loss_bbox, epoch_loss_giou))\n",
    "\n",
    "    # Save every 20 epochs\n",
    "    if (epoch + 1) % SAVE_FREQUENCY == 0:\n",
    "        torch.save(detr_model.state_dict(), f'ckpts/model_epoch{epoch+1}.pt')\n",
    "        np.save(f'ckpts/hist_epoch{epoch+1}.npy', hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the training metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "# Ensure the directory exists\n",
    "os.makedirs(\"cpkts\", exist_ok=True)\n",
    "\n",
    "# Once training is done plot the loss curve...\n",
    "plt.plot(np.log(hist))\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('DETR Loss (log scale)')\n",
    "plt.savefig(\"cpkts/loss_curve.png\")\n",
    "plt.show()\n",
    "\n",
    "# Plot the detailed losses as well...\n",
    "class_losses_plot, bbox_losses_plot, giou_losses_plot = zip(*hist_detail)\n",
    "\n",
    "# Create subplots\n",
    "fig, axes = plt.subplots(3, 1, figsize=(8, 12))\n",
    "\n",
    "# Plot Class Loss\n",
    "axes[0].plot(class_losses_plot, label=\"Class Loss\", color='b')\n",
    "axes[0].set_title(\"Class Loss Over Epochs\")\n",
    "axes[0].set_xlabel(\"Epochs\")\n",
    "axes[0].set_ylabel(\"Loss\")\n",
    "axes[0].legend()\n",
    "axes[0].grid()\n",
    "\n",
    "# Plot BBox Loss\n",
    "axes[1].plot(bbox_losses_plot, label=\"BBox Loss\", color='g')\n",
    "axes[1].set_title(\"BBox Loss Over Epochs\")\n",
    "axes[1].set_xlabel(\"Epochs\")\n",
    "axes[1].set_ylabel(\"Loss\")\n",
    "axes[1].legend()\n",
    "axes[1].grid()\n",
    "\n",
    "# Plot GIoU Loss\n",
    "axes[2].plot(giou_losses_plot, label=\"GIoU Loss\", color='r')\n",
    "axes[2].set_title(\"GIoU Loss Over Epochs\")\n",
    "axes[2].set_xlabel(\"Epochs\")\n",
    "axes[2].set_ylabel(\"Loss\")\n",
    "axes[2].legend()\n",
    "axes[2].grid()\n",
    "\n",
    "# Adjust layout and show plot\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"cpkts/detr_losses.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load fine-tuned model and test inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.visualizer import DETRBoxVisualizer\n",
    "\n",
    "# Trained on your custom dataset\n",
    "detr_model = DETR(\n",
    "    d_model=256, n_classes=len(CLASSES), n_tokens=225, \n",
    "    n_layers=6, n_heads=8, n_queries=100\n",
    ")\n",
    "\n",
    "# Load the checkpoint\n",
    "print(detr_model.load_state_dict(torch.load('../weights/model_epoch20.pt', map_location=torch.device('cpu'))))\n",
    "\n",
    "# Run inference and check results\n",
    "visualizer = DETRBoxVisualizer(class_labels= CLASSES,\n",
    "                               empty_class_id=0)\n",
    "\n",
    "visualizer.visualize_validation_inference(detr_model, coco_ds_val, collate_fn=collate_fn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
